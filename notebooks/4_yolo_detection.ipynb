{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python yoblo.py --image ..\\images\\final_to_match\\HG_140530_003_FW_N20.jpg -y ..\\models\\darknet\\\n",
    "#!python yolo.py --image ..\\images\\NDD20\\data\\3.jpg -y ..\\models\\darknet\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf678b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import skimage \n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "import time\n",
    "\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.io import imread, imsave\n",
    "from pathlib import Path\n",
    "\n",
    "# helper methods for feature extraction\n",
    "from feature_extract import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029b9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "IMG_DIR = '../images/'\n",
    "IMAGES_DIR = IMG_DIR+'final_pigmentation_catalogue_2016'\n",
    "TARGET_IMAGES_DIR = IMG_DIR+'fin_features'\n",
    "MAX_FILES = 50\n",
    "\n",
    "MODEL_DIR=\"../models/darknet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa9b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(img,thrsh_a,thrsh_b):\n",
    "    img_in = img.copy()\n",
    "    xp = [65, 60, 200, 200, 250]\n",
    "    fp = [0, 45, 200, 200, 0]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    img_tmp = cv2.LUT(img_in, table)\n",
    "    \n",
    "    img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2GRAY)      \n",
    "\n",
    "\n",
    "    #blur to reduce noise\n",
    "#     cv2.blur(img_tmp, (18, 18))    \n",
    "#     cv2.GaussianBlur(img_tmp, (11, 11), 0)\n",
    "    \n",
    "    # apply CLAHE\n",
    "    img_tmp = claheGray(img_tmp, 1.5,8)\n",
    "    \n",
    "    # contrast\n",
    "    \n",
    "#     img_tmp = contrast_yt(img_tmp,10,0,255)\n",
    "\n",
    "    # create binary mask and clean away some spots\n",
    "    _,img_bin = cv2.threshold(img_tmp, thrsh_a, thrsh_b, cv2.THRESH_BINARY_INV)\n",
    "#     kernel = np.ones((2, 2), np.uint8)\n",
    "    img_bin = cv2.erode(img_bin, None, iterations=2)\n",
    "    img_bin = cv2.dilate(img_bin, None, iterations=2)\n",
    "#     cv2.imshow(\"cropped\", img_bin)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "    # contour - get the contour from the mask\n",
    "    img_c, cntr, contours = drawContour(img,img_bin,False,200,6,1,45,250,5,7)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
    "\n",
    "    # remove background\n",
    "    cv2.drawContours(img_in, contours, -1, (0,255,0), -1) \n",
    "#     img = cv2.erode(img, None, iterations=1)\n",
    "#     img = cv2.dilate(img, None, iterations=1)\n",
    "#     drawContours( src, contours,largest_contour_index, color, CV_FILLED,8,hierarchy);\n",
    "    return img_in\n",
    "\n",
    "def filter_mask(img,thrsh_a,thrsh_b,canny_a,canny_b,canny_c):\n",
    "    img_in = img.copy()\n",
    "    xp = [65, 60, 200, 200, 250]\n",
    "    fp = [0, 45, 200, 200, 0]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    img_tmp = cv2.LUT(img_in, table)\n",
    "    \n",
    "    img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2GRAY)      \n",
    "\n",
    "\n",
    "    #blur to reduce noise\n",
    "#     cv2.blur(img_tmp, (18, 18))    \n",
    "    cv2.GaussianBlur(img_tmp, (13, 13), 0)\n",
    "    \n",
    "    # create binary mask and clean away some spots\n",
    "    _,img_bin = cv2.threshold(img_tmp, thrsh_a, thrsh_b, cv2.THRESH_BINARY_INV)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    img_bin = cv2.erode(img_bin, kernel, iterations=4)\n",
    "    img_bin = cv2.dilate(img_bin, kernel, iterations=4)\n",
    "    \n",
    "    \n",
    "       \n",
    "    # contour - get the contour from the mask\n",
    "    img_c, cntr, contours,edges = drawContour(img,img_bin,False,200,6,1,canny_a,canny_b,canny_c,7)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
    "\n",
    "    # remove background\n",
    "    cv2.drawContours(img_in, contours, -1, (0,255,0), -1) \n",
    "    img_in = cv2.erode(img_in, None, iterations=1)\n",
    "    img_in = cv2.dilate(img_in, None, iterations=1)\n",
    "    # create mask\n",
    "#     img_mask = np.zeros((img_in.shape[0], img_in.shape[1]), np.uint8)\n",
    "#     cv2.drawContours(img_mask, contours, -1, (255,255,255), -1) \n",
    "#     img_mask = cv2.erode(img_mask, None, iterations=1)\n",
    "#     img_mask = cv2.dilate(img_mask, None, iterations=1)\n",
    "#     img_mask = (255-img_mask)\n",
    "#     cv2.imshow(\"source\", img_mask)\n",
    "    \n",
    "    return img_in, img_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033b71ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 2097152 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8a2010cd2764>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mconfigPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"yolov4-dolphin.cfg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mimg_fins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_fin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweightsPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfigPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCONFIDENCE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTHRESHOLD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_fins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\UNI\\dolphin_classification\\notebooks\\feature_extract.py\u001b[0m in \u001b[0;36mfind_fin\u001b[1;34m(image, weightsPath, configPath, CONFIDENCE, THRESHOLD, img_size, pad_col)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[0mlayerOutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m     \u001b[1;31m# show timing information on YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 2097152 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "# image = cv2.imread(\"..\\\\images\\\\final_to_match\\\\HG_140530_003_FW_N20.jpg\")\n",
    "image = cv2.imread(IMG_DIR+\"NDD20/data/5.jpg\")\n",
    "# cv2.namedWindow(\"source\", cv2.WINDOW_AUTOSIZE)\n",
    "# imS = cv2.resize(image, (960, 540))  \n",
    "# cv2.imshow(\"source\", imS)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyWindow(\"source\")\n",
    "# image = resizeAndPad(image, (720,1000), 255)\n",
    "cv2.namedWindow(\"source\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "CONFIDENCE = 0.3\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "weightsPath = os.path.sep.join([MODEL_DIR, \"training/yolov4-dolphin_best.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_DIR, \"yolov4-dolphin.cfg\"])\n",
    "\n",
    "img_fins, _ = find_fin(image,weightsPath,configPath,CONFIDENCE,THRESHOLD,IMG_SIZE,0)\n",
    "\n",
    "print(len(img_fins))\n",
    "\n",
    "img_original = img_fins[0]\n",
    "# add border\n",
    "img = add_border(img_original,5,0)\n",
    "\n",
    "alpha = 1.0\n",
    "alpha_max = 500\n",
    "beta = 0\n",
    "beta_max = 200\n",
    "gamma = 1.3\n",
    "gamma_max = 200\n",
    "\n",
    "\n",
    "bin_thrsa = 85\n",
    "bin_thrsb = 240\n",
    "\n",
    "canny_a = 0.75\n",
    "canny_b = 0.5\n",
    "canny_c = 0.05\n",
    "\n",
    "MASK = False\n",
    "def basicLinearTransform():\n",
    "    resa = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "#     print(canny_a)\n",
    "    lookUpTable = np.empty((1,256), np.uint8)\n",
    "    for i in range(256):\n",
    "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    img_correcteda = cv2.LUT(resa, lookUpTable)\n",
    "    if not MASK:\n",
    "        img_correctedb, img_mask = filter_mask(img_correcteda,bin_thrsa,bin_thrsb,canny_a,canny_b,3)\n",
    "    img_corrected = cv2.hconcat([img, img_correctedb])\n",
    "    cv2.imshow(\"adjustments\", img_corrected)\n",
    "\n",
    "def on_linear_transform_alpha_trackbar(val):\n",
    "    global alpha\n",
    "    alpha = val / 100\n",
    "    basicLinearTransform()\n",
    "\n",
    "def on_linear_transform_beta_trackbar(val):\n",
    "    global beta\n",
    "    beta = val - 100\n",
    "    basicLinearTransform()\n",
    "\n",
    "def on_gamma_correction_trackbar(val):\n",
    "    global gamma\n",
    "    gamma = val / 100\n",
    "    basicLinearTransform()\n",
    "    \n",
    "def on_bina_adjust_trackbar(val):\n",
    "    global bin_thrsa\n",
    "    bin_thrsa = val\n",
    "    basicLinearTransform()\n",
    "\n",
    "def on_binb_adjust_trackbar(val):\n",
    "    global bin_thrsb\n",
    "    bin_thrsb = val\n",
    "    basicLinearTransform()\n",
    "    \n",
    "def on_canny_a_adjust_trackbar(val):\n",
    "    global canny_a\n",
    "    canny_a = val\n",
    "    basicLinearTransform()\n",
    "    \n",
    "def on_canny_b_adjust_trackbar(val):\n",
    "    global canny_b\n",
    "    canny_b = val\n",
    "    basicLinearTransform()  \n",
    "    \n",
    "def on_canny_c_adjust_trackbar(val):\n",
    "    global canny_c\n",
    "    canny_c = val\n",
    "    basicLinearTransform()\n",
    "    \n",
    "def setMask(*args):\n",
    "    pass\n",
    "\n",
    "img_corrected = np.empty((img.shape[0], img.shape[1]*2, img.shape[2]), img.dtype)\n",
    "img_gamma_corrected = np.empty((img.shape[0], img.shape[1]*2, img.shape[2]), img.dtype)\n",
    "\n",
    "cv2.namedWindow(\"adjustments\", cv2.WINDOW_AUTOSIZE)\n",
    "# cv2.namedWindow('Gamma correction')\n",
    "\n",
    "alpha_init = int(alpha *100)\n",
    "cv2.createTrackbar('Alpha gain (contrast)', 'adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\n",
    "beta_init = beta + 100\n",
    "cv2.createTrackbar('Beta bias (brightness)', 'adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\n",
    "gamma_init = int(gamma * 100)\n",
    "cv2.createTrackbar('Gamma correction', 'adjustments', gamma_init, gamma_max, on_gamma_correction_trackbar)\n",
    "\n",
    "bina_init = int(bin_thrsa)\n",
    "cv2.createTrackbar('Binary Threshold a', 'adjustments', bina_init, 255, on_bina_adjust_trackbar)\n",
    "binb_init = int(bin_thrsb)\n",
    "cv2.createTrackbar('Binary Threshold b', 'adjustments', binb_init, 255, on_binb_adjust_trackbar)\n",
    "\n",
    "# canny_a_init = int(canny_a*100)\n",
    "# cv2.createTrackbar('Canny Threshold a', 'adjustments', canny_a_init, 255, on_canny_a_adjust_trackbar)\n",
    "# canny_b_init = int(canny_b*100)\n",
    "# cv2.createTrackbar('Canny Threshold b', 'adjustments', canny_b_init, 255, on_canny_b_adjust_trackbar)\n",
    "# canny_c_init = int(canny_c*100)\n",
    "# cv2.createTrackbar('Canny Threshold c', 'adjustments', canny_c_init, 7, on_canny_c_adjust_trackbar)\n",
    "\n",
    "#cv2.createButton(\"Set Maks\",setMask,NULL,cv2.QT_PUSH_BUTTON,1)\n",
    "\n",
    "\n",
    "on_linear_transform_alpha_trackbar(alpha_init)\n",
    "on_gamma_correction_trackbar(gamma_init)\n",
    "on_bina_adjust_trackbar(bina_init)\n",
    "on_binb_adjust_trackbar(binb_init)\n",
    "# on_canny_a_adjust_trackbar(canny_a_init)\n",
    "# on_canny_b_adjust_trackbar(canny_b_init)\n",
    "# on_canny_c_adjust_trackbar(canny_c_init)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c62425",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)\n",
    "# image = cv2.imread(\"..\\\\images\\\\final_to_match\\\\HG_140530_003_FW_N20.jpg\")\n",
    "image = cv2.imread(\"../images/NDD20/data/5.jpg\")\n",
    "# image = resizeAndPad(image, (720,1000), 255)\n",
    "\n",
    "CONFIDENCE = 0.3\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "weightsPath = os.path.sep.join([MODEL_DIR, \"training\\\\yolov4-dolphin_best.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_DIR, \"yolov4-dolphin.cfg\"])\n",
    "\n",
    "img_fins = find_fin(image,weightsPath,configPath,CONFIDENCE,THRESHOLD,IMG_SIZE,0)\n",
    "\n",
    "print(len(img_fins))\n",
    "for img in img_fins:\n",
    "    # make gray\n",
    "    cv2.imshow(\"cropped\", img)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    # add border\n",
    "    img = add_border(img,5,0)\n",
    "    \n",
    "#     img_cntr = contrast_yt(img,100,0,255,1)\n",
    "    yt = threshold_yen(img)\n",
    "#     img_cntr = rescale_intensity(img, (0, yt), (0, 255))\n",
    "    \n",
    "    \n",
    "    alpha = 1.0 # Simple contrast control\n",
    "    beta = 0    # Simple brightness control\n",
    "    gamma = 0.5    \n",
    "    lookUpTable = np.empty((1,256), np.uint8)\n",
    "    for i in range(256):\n",
    "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    img_gamma = cv2.LUT(img, lookUpTable)\n",
    "    #img_gamma = cv2.hconcat([img, res])\n",
    "    \n",
    "    cv2.imshow(\"cropped\", img_gamma)\n",
    "    cv2.waitKey(0)   \n",
    "    \n",
    "#     image = image.astype('float32')\n",
    "    img_tmp = img_gamma\n",
    "    #contrast stretching\n",
    "    xp = [65, 80, 200, 200, 250]\n",
    "    fp = [0, 45, 200, 200, 0]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    img_tmp = cv2.LUT(img_tmp, table)\n",
    "    \n",
    "    img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2GRAY)      \n",
    "#     cv2.imshow(\"cropped\", img_tmp)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.bilateralFilter(img, 3, 50, 50)\n",
    "\n",
    "    #blur to reduce noise\n",
    "#     cv2.blur(img_tmp, (18, 18))    \n",
    "#     cv2.GaussianBlur(img_tmp, (11, 11), 0)\n",
    "    \n",
    "    # apply CLAHE\n",
    "    img_tmp = claheGray(img_tmp, 1.5,8)\n",
    "    \n",
    "    # contrast\n",
    "    \n",
    "#     img_tmp = contrast_yt(img_tmp,10,0,255)\n",
    "\n",
    "    # create binary mask and clean away some spots\n",
    "    _,img_bin = cv2.threshold(img_tmp, 100, 240, cv2.THRESH_BINARY_INV)\n",
    "#     kernel = np.ones((2, 2), np.uint8)\n",
    "    img_bin = cv2.erode(img_bin, None, iterations=2)\n",
    "    img_bin = cv2.dilate(img_bin, None, iterations=2)\n",
    "#     cv2.imshow(\"cropped\", img_bin)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "    # contour - get the contour from the mask\n",
    "    img_c, cntr, contours = drawContour(img_gamma,img_bin,False,200,6,1,45,250,5,7)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
    "\n",
    "    # remove background\n",
    "    cv2.drawContours(img_gamma, contours, -1, (0,0,0), -1) \n",
    "#     img = cv2.erode(img, None, iterations=1)\n",
    "#     img = cv2.dilate(img, None, iterations=1)\n",
    "#     drawContours( src, contours,largest_contour_index, color, CV_FILLED,8,hierarchy);\n",
    "\n",
    "    cv2.imshow(\"cropped\", img_gamma)\n",
    "#     cv2.imshow(\"cntr\", cntr)\n",
    "    cv2.waitKey(0)\n",
    "    exit\n",
    "        \n",
    "#             crop_img = detectFin(crop_img)\n",
    "# show the output image\n",
    "# cv2.imshow(\"Image\", image)\n",
    "# crop_img = resizeAndPad(crop_img, (512,512), 255)\n",
    "# cv2.imshow(\"cropped\", crop_img)\n",
    "# cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
